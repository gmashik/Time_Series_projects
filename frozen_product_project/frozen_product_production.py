# -*- coding: utf-8 -*-
"""Frozen_product_production.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1R6GCTt_H_z5AgNNeq1bGhYDnf5JkYazB

# **Frozen product production project**

<table align="center">
  
  <td align="center"><a target="_blank" href="https://colab.research.google.com/drive/1R6GCTt_H_z5AgNNeq1bGhYDnf5JkYazB?authuser=2#scrollTo=YDoUbJbWKJB9">
        <img src="https://encrypted-tbn0.gstatic.com/images?q=tbn%3AANd9GcTR7EtP27gljpJg91k2DVoRgkB84hkMl78bOA&usqp=CAU""  style="padding-bottom:5px;" />
        
  Run this project in Google Colab</a></td>
  
</table>

#DATA INFO
Info about this data set: https://fred.stlouisfed.org/series/IPN31152N
Units:  Index 2012=100, Not Seasonally Adjusted
Frequency:  Monthly
The industrial production (IP) index measures the real output of all relevant establishments located in the United States, regardless of their ownership, but not those located in U.S. territories.

Suggested Citation:
Board of Governors of the Federal Reserve System (US), Industrial Production: Nondurable Goods: Ice cream and frozen dessert [IPN31152N], retrieved from FRED, Federal Reserve Bank of St. Louis; https://fred.stlouisfed.org/series/IPN31152N, November 16, 2019.
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_squared_error
import tensorflow as tf
# %matplotlib inline

!wget https://raw.githubusercontent.com/gmashik/Time_Series_projects/master/frozen_product_project/data/IPN31152N.csv

df = pd.read_csv('IPN31152N.csv',index_col='DATE',parse_dates=True)

df.head(3)

df.columns = ['Production']

df.head(3)

df.plot(figsize=(16,8))
plt.grid()

"""## **Train test data split**"""

test_ind = len(df)-int(len(df)*10/100)
train = df.iloc[:test_ind]
test = df.iloc[test_ind:]

scaler=MinMaxScaler(feature_range=(0,1))
scaler.fit(train)
scaled_train = scaler.transform(train)
scaled_test = scaler.transform(test)

window_size = 18
n_features=1
generator =tf.keras.preprocessing.sequence.TimeseriesGenerator(scaled_train, scaled_train, length=window_size, batch_size=1)

"""## **Define model**"""

tf.keras.backend.clear_session()

model = tf.keras.models.Sequential([
  tf.keras.layers.LSTM(64 ,activation='relu', input_shape=(window_size, n_features)),
  tf.keras.layers.Dense(1),
])
model.compile(optimizer='adam', loss='mse')

validation_generator = tf.keras.preprocessing.sequence.TimeseriesGenerator(scaled_test,scaled_test, length=window_size, batch_size=1)

early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss',patience=2)
# fit model
model.fit_generator(generator,epochs=20,
                    validation_data=validation_generator,
                   callbacks=[early_stop])

loss = pd.DataFrame(model.history.history)
loss.plot()

predictions = []

first_batch = scaled_train[-window_size:]
model_batch = first_batch.reshape((1, window_size, n_features))

for i in range(len(test)):
    pred = model.predict(model_batch)[0]
    predictions.append(pred) 
    model_batch = np.append(model_batch[:,1:,:],[[pred]],axis=1)

inversed_predictions = scaler.inverse_transform(predictions)

test['Predictions'] = inversed_predictions

test.tail(5)

"""## **Plot predictions versus the True test values**"""

fig=plt.figure(figsize=(18,6),dpi=500)
ax=fig.add_axes([0,0,1,1])
test.plot(ax=ax,linewidth=2,linestyle='--')

print("Mean squared error: ",np.sqrt(mean_squared_error(test['Production'],test['Predictions'])))

"""### **The prediction is not very bad. So move with this to retrain the model and make some forcast**"""

full_scaler = MinMaxScaler(feature_range=(0,1))
scaled_full_data = full_scaler.fit_transform(df)
window_size = 12 # Length of the output sequences (in number of timesteps)
generator = tf.keras.preprocessing.sequence.TimeseriesGenerator(scaled_full_data, scaled_full_data, length=window_size, batch_size=1)

tf.keras.backend.clear_session()

model = tf.keras.models.Sequential([
  tf.keras.layers.LSTM(64 ,activation='relu', input_shape=(window_size, n_features)),
  tf.keras.layers.Dense(1),
])
model.compile(optimizer='adam', loss='mse')
model.fit_generator(generator,epochs=10)

forecast = []
# Replace periods with whatever forecast length you want
periods = 24

first_eval_batch = scaled_full_data[-window_size:]
current_batch = first_eval_batch.reshape((1, window_size, n_features))
for i in range(periods):
    current_pred = model.predict(current_batch)[0]
    forecast.append(current_pred) 
    current_batch = np.append(current_batch[:,1:,:],[[current_pred]],axis=1)

forecast = scaler.inverse_transform(forecast)

forecast.shape

df.iloc[-1]

forecast_index = pd.date_range(start='2020-07-01',periods=periods,freq='MS')
forecast_df = pd.DataFrame(data=forecast,index=forecast_index,
                           columns=['Forecast'])

forecast_df

fig=plt.figure(figsize=(18,6),dpi=1000)
ax=fig.add_axes([0,0,1,1])
df.plot(ax=ax,linewidth=2)
forecast_df.plot(ax=ax,linewidth=3,color='r',linestyle='--')

"""# **Conclusion**

**Time series data is very difficult to predict. However our simple model grasp a very good projection on future data. More architecture tuning can be used a achieve more accurate prediction.**
"""

